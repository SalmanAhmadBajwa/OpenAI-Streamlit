{"podcast_title": "The Backend Engineering Show with Hussein Nasser", "episode_title": "The Journey of a Request to the Backend", "episode_image": "https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/222061/222061-1626111277047-d82a050054721.jpg", "episode_transcript": " I'd like to take you for a journey of the request, which is this unit of work that the front end sends to the back end. Exactly what happened from the moment that this request is sent to until it reaches the back end user space process for processing. It's often the case that we focus on, I suppose, the first step sending the request and the last step actually processing that request. When you get that on request beautiful event in Node.js or Python, hey someone just sent the request. That's all what you see in the back end process. But there is so much to it than that. There are so many things that happen in between. And while we'd like sometimes to, it's nice to close our eyes and just wish that this magic happens in the most performant way. It doesn't always happen. Just because these things are hidden from us using TLS libraries, OpenSSL and HTTP libraries if you're using HTTP as a protocol and the kernel and it's all it's and it's all it's mighty capabilities. To be honest, it's hiding these things from us. But just because these things are hidden, they do not mean that they don't exist. There's so much work involved and this really understanding in it is what I tend to push in this channel. Right? It's like you don't have to code everything, right? Build everything yourself. That's not what I'm saying. It's just you can use these libraries. But I think back end engineers specifically, full stack, no, mandatory, we have to understand this thing. We need to understand exactly what happens at every stage of the lifetime of a request. Because we tend to take it for granted. And then when things go wrong, we feel paralyzed, right? Because we don't understand what's happening. When we couldn't scale beyond a thousand connections per machine, we tend to just throw more hardware at it. Or just spin up more machines or go distributed. Primitrually, might I say. Adding more hardware without actually understanding the cause of the bottlenecks here. And I talk about that in my new course, my performance, unlocking the back end performance, bottlenecks. So many things happen until the request is ready to be processed. And I'm here to kind of go through that journey, kind of illuminate these steps. But before we get started, we need to define what a request really means. So we need to define what a request really means. Right? So how about we get started. Welcome to the backend engineering show with your host, Hussein Nasser. This is the show where we discuss the art and the craft of building software and cover recent news on backend technology. And we're going to be talking about the backend engineering and the backend engineering and the backend engineering. This is the show where we discuss the art and the craft of building software and cover recent news on backend technologies. If you enjoy the show, make sure to subscribe to the YouTube channel Spotify and Apple podcast. With that said, let's get on the show. If it wasn't for Spotify for podcasters, you won't be listening to this episode of the backend engineering show. Spotify for podcasters is the easiest way to start a podcast with everything you need in one place. It lets you record and edit podcasts right from your phone or computer. And that is exactly how I did it actually in the first hundred episode of the backend engineering show, you can distribute your podcast to Spotify and everywhere else podcasts are heard. Nothing's more convenient than that. And if you prefer video form, Spotify got that cover too. I think one of my most popular episode actually are in video form. You can also monetize your work and earn in a variety of ways, including ads and podcast subscriptions. And best of all, Spotify for podcasters is totally free. Download the Spotify for podcasters app or go to www.spotify.com slash podcasters to get started. Welcome to backend engineering show with your host Hussein Nasr. And a request is a unit of work that is submitted by a front end of some sort in a form of a specific well-defined protocol. And this protocol often sits on top of either TCP or UDP, which is the, you know, the de facto protocol of transport in the internet. Right. You might say, why do I, do I, do I need, why do I need that? Why do I need another protocol to define requests? Can't I say, Hey, my requests are always 10 by long. And if I send them, they always start with this particular binary, binary code. And this is their end. Right. And they are always 10, 10 bytes in length. Well, you just defined a protocol essentially. Right. Like you defined your own application protocol that segments the, the, the best stream of bytes. Cause you see, always think of the TCP layer at least, no, as a, as a hose, it's a water hose. It's just like someone's pointing a constant stream of water to a machine and just, it's just a stream of bytes. Right. And that machine just reads, retreats these bytes. It doesn't have any meaning per se, but then you define meaning to it in the, in the application by actually reading these streams, you're reading, reading, reading, reading, reading, reading and says, Oh, here's a start of a request. And you define what that, what does that mean? Right. Here's, Oh, I see a head of a request. Like that's the head. It's like, okay, I'm reading that. Oh, nevermind. That's just a false alarm. Just toss it. It's like, Oh, actually this is a request. It's a get space slash HTV one one. Oh, it looks like an HTV one one request. All right. Let me continue reading. Oh, I see content length header. Okay. Continue. It says like here it's a thousand byte length. So I need to read right after I finished the headers. I need to read a thousand bytes after the headers. That's my body of the request. It has to be a post in this case because gets don't have buddy. But then that is essentially my request. And that is as a unit, you probably de serialize that right into the language of choice that you choose C plus, plus C, Python, Ruby, no GS, whatever. And that gets you a nice object in the back end. It says, Hey, Rick, R E Q and that's your request. But there's so much work that happened there. And, and, and, and essentially it's just defining the first request, the start of the request and the end of the request. But guess what? The backend needs to read the stream of byte and always do this parsing thing that we talked about to find out the start and the end of a request and chunk those buys into meaningful unit of work. And HTTP one, one as a protocol is different than HTTP two and it's different than HTTP three as a protocol when it comes to the on wire representation of a request, completely different. HTTP one, one is the simplest, most elegant representation of that. If you receive something from HTTP one, one protocol, that is the request. There is no, you know, other stuff to it. You know, it's, it's, it's straight to the point while HTTP two, no, there are frames, there are, there are, there's an additional structure that is wrapped around requests. There are streams and streams have frames and there is a data frame and there is a header frame and there is other type of frames. So there's more work involved in parsing HTTP two than HTTP one. And that's why HTTP two is more CPU intensive because your backend process, believe it or not, is doing stuff that it didn't used to do in HTTP one, one. Right. And then there is also your backend is referencing an open SSL or Libra SSL library to do the decryption and encryption for TLS sessions, right? That's also an untrivial cost, right? Before you even start to do the that before you even start parsing all that stuff, you need to, to decrypt these bytes before you even begin to parse, right? That's, that's the problem that we're faced with that there's so much happening. So that's a request. Now let's go through these steps. And now that we define what a request is a request, like to summarize, a request has a start and a beginning as has a beginning and an end, and this defined by your protocol. So even SSH has a request. If you do an anything like LS and you hit enter, right? In a bash environment, SSH, you are sending a request and you're waiting for a response from the server, right? And that that's also has a unique definition. So let's go through these stages. These steps goes through six stages. A request goes through six stages before we even send a request. We need a vehicle to send the request on and that's called the connection. It's a, it's customary to establish a connection between a client and a server. And it's often a TCP connection. So you do a sense and an ACK, right? And then you have a connection. That means you, you as a front end and the server, the back end have agreed on certain situations. You agreed on the window sizes, you agreed on the sequences, and now you can start labeling your bytes, not request. TCP does not know anything about requests. Just bytes to it, right? Now you use these bytes, this stream of bytes as a vehicle to stop sending your request in whatever language you want, whatever protocol you want. But before you actually build that connection, what is really happening here? So let's talk about, let's explore the connection space and how the kernel is involved and how the backend is involved as well. So when you first listen on this port 8080, TCP, and you listen on a specific interface, right? Specific IP address. When you do that listening, the kernel will create two queues for you, just for the listener. Right? So the 8080 listener, it will create a socket object. And that socket object is a file descriptor. Everything in Linux is a file. And that is associated with port 8080 and that is the destination IP is this. And you can also listen on all interfaces by doing 0.0.0.0, or that's IPv4, all IPv4 interfaces. If you want to listen to all IPv6 interfaces, you do colon, colon, double colons, and that will listen on all IPv6. Not always a good idea, because now your attack surface just unnecessarily expanded. Just listen on the things you actually need to listen on. That's always a good idea. Now that file descriptor points to this port and then it has this IP address. Let's give it an IP address. 1, 2, 3, 4. The kernel also creates two pieces of data structure that is associated with this socket. Essentially, they are pointers. And this is essentially called the SYN queue and the accept queue. The accept queue holds the full-fledged connections, while the SYN queue holds the SYNs that are arrived to this particular socket. And they are held often temporarily until we... And this is all in the kernel. The backend is not even involved with any of this stuff. All you do is just basically say listen and the kernel creates a data structure for you. Two data structures, two queues in this case. So now let's say a client wants to connect to this 1, 2, 3, 4 port 8080. It will send a SYN. That packet will go all the way to the NIC, the Network Interface Controller, or card. And that SYN will eventually... It's a packet, a CTV packet. It will move from the NIC to the kernel memory via a process often called DMA, which is the direct memory access, where the data is moved directly from that to the kernel. CPU is not often involved in that area. The kernel directly reads data from the network memory, network card memory, or you can actually flip it that the network card actually writes to the kernel directly. So now the kernel finds out that, oh, we got a SYN. Where is it addressed to? Is it addressed to 1, 2, 3, 4 port 8080? Guess what? It will do a lookup. It says, oh, actually I have a queue for that particular socket. If it doesn't have it, it will drop it. It will drop that packet. And well-behaved kernels will actually reply back with an ICMP message saying, destination unreachable, port unreachable, or whatever. But if the packet does exist, then it will put that SYN into that SYN queue for that backend. Again, this is all in the kernel. And the kernel immediately replies back to the client with a SYNAC to complete the connection. And now it continues to wait. It says, all right, I sent the SYNAC. Now once the client acts the SYNAC, and we get back an ACK, that is when I have a full-fledged connection. That's the three-way handshake, the TCP. Now I can move that full-fledged connection to the accept queue, which is another queue also in the kernel. And we put it there. And now the connection lives there. Is it still ready to be consumed? Not yet. It's now us, the backend application, responsible for accepting the connection. We have to accept the connection. And that's literally a system call. You call accept on the socket file descriptor and says, hey, accept, accept, accept. It says, hey, whatever, if there is a connection, I want to accept it. When you call accept, if there is a connection in the accept queue, the kernel will pop it and it will lock the structure. Pop it and then return a new file descriptor representing that connection to the backend application. Now the backend has a pointer, if you wish, to the connection. Now the backend can talk directly to the client via this file descriptor. We almost established this connection. There's now a line connecting the client all the way to the backend. Nice. We have a connection. How large is this accept queue? I can go into details. You can actually specify how large this accept queue can get because guess what? If the backend doesn't accept connections, then the connections will remain in the accept queue before the client accepts them. But for how long? Right? Well, until the accept queue is full. Well, what is the size of the accept queue? Well, you determine it as the backend application. When you listen, you actually specify a parameter called backlog. And the backlog can specify how many connections can live there unacceptable before new SIN will start to be timed out. Because, hey, my accept queue is full. I can't accept new connections. I cannot complete, not accept. I cannot as a kernel, I cannot complete new connections. Nice. All right. First step, accept. So there is just that step. Right? There is so much work, right? That you as the backend engineer need to understand because your challenge here is to support a lot of connections. First, you have to learn how to accept connections as fast as possible. Because if the backlog accept queue size is 128, then if you receive a thousand connections, that backlog is going to be filled. And if you have a single thread accepting connection, dedicated for accept connection, it cannot possibly, you will reach a point where that accept queue will be so small and you will increase it. But then the problem is you still have a lot of backlog connections to be accepted. And now you're forced to spin up multiple threads to accept. I talked about all of that in my new course as well. There is tricks and challenges and you can literally have your backend spin up two threads or two processes even. Both listen on the same port. You might say, no, that's not right. You're going to get this error, address and use. No, not if you flag the listener with a flag called socket option reuse port. And if there is a specific handshake between you two as processes, you can theoretically have 16 threads listening on the same port. So if you do that, then you get 16 accept queues and 16 sin queues. Do you get 16 sin queues? I don't know. You're probably going to get one sin queue and 16 accept queues. And now once you receive connections, the kernel load balance those connections to the 16 queues and the threads can each read from their own accept, from their own accept queue essentially. As I suppose as multiple threads fighting to read from the single accept queue because you're going to get limited by the accept mutex. It's basic, you know, some of our ideas have to lock stuff. Otherwise you get race condition. That's bad. All right. So we talk so much about the accept, but now eventually we have, there's so much you can do here with the exception to accept a connection. Step number two, our work is not even begin. We just have a connection now. All right. The step number two is actually, you got to read buddy. As a backend application, you now you have a connection. The client is going to send you stuff. Who's going to read? Well, it's you. The backend application is going to read or receive RCV, I suppose. But what happened here? What really, really happened? And what are we reading? You're reading, and let's say, let's say it's port 844, 443. So it's encrypted. Let's spice things up a little bit. Let's add encryption to the mix. So the client will send requests. They are one after the other, right? Whether they are in streams or, right, or other format, they'll be sent. And those will be encrypted with DLS. So the, at the end of the day, once you accept a connection, I forgot to mention that you get additional two queues per connection. One is called the X, the receive queue. And one is the send queue. For each connection that you create, you're going to get a receive queue where the bytes will arrive from the client. And you're going to get a send queue where your bytes as a backend application will be moved to the send queue before it's transmitted across the network, right? And guess what? Let's talk about just the receive queue. So if a client sends an HTTP request, right, and it's encrypted, those are bytes, right? The Nick will receive those bytes, as we talked about. And the kernel will identify that, oh, these bytes are going to this, to this IP, to this port, and to this part from this particular client, from this particular port. So and I guess what, these four pairs essentially map to a unique connection. And here it is. It is this receive queue. It's all pointers. It's like, oh, so let me just move this SKB, I think that's called socket buffers, right? So that I move this socket buffer to this receive queue, to this connection buffer queue. Now you have a connection that you as the backend application got the file scripted for. And it has a receive queue with data, but you don't see that yet. That receive queue keeps receiving data and just accumulating the receive queue. And those packets are acknowledged by the kernel. All the TCP stuff is done by the kernel. That was work done a long time ago to actually move the TCP layer down to the kernel, as opposed from the kernel space, sorry, the user space. Quick today is being done in the user space, right? As far as the kernel is concerned, it's all UDP. It doesn't know it's quick or not. Maybe it does, but it doesn't do anything about it. In the future, maybe. But today, we're acknowledging all this, you know, window sizing and controlling the window sizes and shrinking and increasing and then the congestion control algorithms and the slow start. All of this is implemented in the kernel. And with the users, you can specify change parameters, of course, but that's what you have. You have now the receive queue with a bunch of bytes. Now, as a backend application, you call read. Again, you might say, Hussein, I never in my life did accept or called read or anything. Well, the framework or the library or the language often does that for you. So yeah, your backend is reading bytes. But guess what? Those bytes are raw, raw encrypted bytes. And that you read, that backend reads. And you have a buffer and you read them. And the backend associates specific memory, either in the stack or the heap, depends how you do it. Like if you declare a variable in your function, that is a stack. Right. But if you did a malloc, then that's a heap. So this data is copied from the kernel memory, receive queue down to the user space memory. And that's why IOU rank, the new interface in Linux tries to eliminate this unnecessary copying back and forth between kernel space and memory user space. But yeah, you get, you get, you know, you copied encrypted data. Guess what? What do you do with this? They are absolutely useless to you. They don't represent a request. Up until the step, we don't know what a request is yet. So we're step number one, we accept the connections to remember to be reading bytes. We have absolutely no idea what a request is. We're just blind as a backend application. So now you're reading bytes and those bytes will be copied. As we said, these encrypted bytes will be copied to the memory of the user space to your backend application essentially. And they live there. Another challenge is, is reading as well. Like how fast you read. Cause that receive buffer also have a size, a limited size. So you as a backend application must read fast, as fast as the client can send. And the other way around, if you send data, the client should be able to handle what you sent. It's a, it's a delicate tug of war kind of a situation here. So that's another thing you can investigate and invest in and understand the architecture of multiple readers. So you can have multiple acceptors, one acceptor up to you like not no JS is a one acceptor, one reader, right? Because it's a single thread and everything happens in that thread. Literally unless you, you do asynchronous work, then multiple threads will be used. But yes, that's why we're like every language you pick, every framework you pick, it's, it's, it's going to do the thing differently, but the fundamentals do not change. They, you have to do these things. Right? If there's a fire, there must be smoke. And now that's the second step. Read. If things were not encrypted, right? We're probably going to be almost done here. We're going to read and then we're going to start the process of parsing, right? But we can't parse anything yet in the back. The protocol cannot kick in yet. Our protocol, the HTTP or the SSH or whatever protocol you build or WebSocket, nothing can kick in yet. And step two, we just reading, encrypted, but if it's unencrypted, yeah, we can start the process of parsing the protocol and finding our request, but not yet. Right? So now move us to step number three, which is decrypting. Now, now that I have in my memory, a bunch of a bag of encrypted bytes that I know they are encrypted because I did a session, I did a CTLS session. Now I find my key, the, you know, the symmetric key that I use to decrypt and I exchange with a client. That's the whole thing. You know, did I put it in a memory? Probably sit there and you'll use that space memory. Let's just pull it up and decrypt it, you know, with it. If you're sophisticated enough, you probably like the private key and the certificates and all this stuff is going to be in a TPM somewhere in your heart, you know, on your motherboard. You're going to ask your TPM to do the decryption, right? Or at least this session establishment for you because you don't use the private key to decrypt, you know, traffic. You just use it to sign stuff more likely. The session key is probably living somewhere in your user space memory, which is, I suppose you can say it's dangerous, but that's what we do today, right? Like if another heart bleed happened and it's, it's unlikely because these session keys are ephemeral. Once you're done with the connection, you drop the session key. So it's not like it's going to live forever like the private key. So you're not going to get a heart bleed situation here. But yeah, you can, there's another thing the security folks are taking care of thinking about all these problems. So step number three to decrypt, you take the session key and then you decrypt these files and guess what? You have to copy them in another location, right? Now you occupy double the memory, essentially. Can you decrypt in place? I don't know. Maybe, but now you're occupying memory, more memory. You need more memory to equip by from decrypt from this and this. And you often, you don't do this yourself as well. You use a library and, and even, you might even don't know any of that stuff because you use an HTTP library or even node, right? Or Python that uses HTTPS protocol that uses either whatever is installed in your drive is open SSL or Libre SSL or whatever. And that library is probably linked with your, right? And the specific virtual page in your process. And then you use that to do the decryption. So the code is, is already on your machine. I just use that to decrypt. So, so that code gets kicked in, but it's, it's the cost that it's your process that's done, does the execution because you, it's, it's that library opens, it is mapped, it's linked to your process, right? So it's as if you are doing the back and you might come back and say, Hey, what's this? What's this? What's this? What's this? What's this? What's this? What's this? And you might come back and say, Hey, why is my backend process taking like 98% CPU? I don't do anything. I'm not doing anything. Well, you are doing a lot of things. It's just, you don't know about it, right? So all this crypto magic, you know, AES 120, 256 signature signing, you know, digest all of this happens in step number three, decrypting. Now that we can toss the encrypted stuff, deallocated, now we have a nice memory of unencrypted stuff. Step number three, we decrypted, we took another hit, right? Now this is the first time we actually take a CPU hit per se. Step number two wasn't really, we didn't really need anything CPU. We just literally did a copy, right? It's an IO. And we don't do any processing per se in step two. Maybe in step one, even step one is just an exception, like except we're accepting stuff. So that's the first step we're actually using the CPU. So your process becomes slightly CPU bound if it's TLS, if there's TSLS involved, but watch out for that. Step number four, now that we have a bunch of unencrypted bytes, we begin parsing. We know our backend is an HTTP protocol or let's say it's HTTP 1.1 or HTTP 2 or HTTP or SSH or Postgres or MySQL protocol. Now we begin actually parsing. By the way, this step is identical for any backend. Talk about database, web server, a live streaming web server, you know, any backend must do these steps, right? So parsing is another step here, which is based on my protocol. I'm going to parse now. So all right, here's where my request starts. Oh, there you go. Here's a request. I have this much amount of byte. If we're lucky and whatever I have in my memory so far, I begin parsing and I might find a request in this hunk of bytes, right? Or might be even lucky. I find two requests. Oh, there's one request and there's another request or even luckier. I have three requests. Isn't that cool? Three requests in one shot. Well, which one gets processed first? That's a decision to the backend. Let's not get there. Again, that's why I am fascinated by backend engineering because there is no one way to do any of this stuff. You can do it anywhere, anywhere you want. That is always pros and cons to anything. But yeah, or the bad side is you might be an unlucky bastard. And the memory that you just read doesn't have a half a request. It happens, right? Because the request is still large. So you happen to accept read, but you only read half of worth of data. You decrypted that half of worth, but that request is in another half, or maybe it still is in the receive queue that you didn't read, or maybe the client sent it and it didn't arrive. So now you're in this forcing hell. So this is now an incomplete request. What do you do with that? Well, you put it on a side because you have other stuff to take care of. Well, what do you do? Really? Do you wait for more data to arrive and hopefully the request will be completed? You have to because it's an incomplete request. You cannot fulfill that request. You have to because it's an incomplete request. You cannot fulfill an uncompleted request. Well, unless you're uploading some file and you got enough information from the first request that you know that the remaining is actually the data. So you can do tricks with that. But most of the case, if you do that, then the backend will be waiting for more data to be read and decrypted before it can actually parse this request. You know what I mean by parse. Just find out the start and the end. And sometimes not any data that available there is actually a request. It could be just system frames from an HTTP to Oracle like settings or window update. That's where it's like, OK, I can't do anything. That's not a request. That's not an actionable back in request. And here is where the second hit to your CPU is performed on parsing. Because now it's all CPU. You're just taking the memory and then you're reading, reading, reading, looking for comparing certain things, finding specific headers, finding specific bodies, finding specific patterns for your request. So it's all CPU and HTTP 1.1 is going to give you the least amount of headache here because it's a very vanilla, straight protocol. It doesn't have any magic to it. But HTTP 2, well, there's a lot of other headers and binary frames, WebSockets, same thing has its own headers that you need to unpack and understand. So you're using this library that you hope someone wrote in a very efficient manner that parses this stuff. So you take another hit of the CPU and up until today, 2023, it's known that HTTP 2 consumes more CPU than HTTP 1.1. Now, it's known that HTTP 2 consumes more CPU than HTTP 1.1. And just because the logic, HTTP 2 does more. Well, it does give you benefits that you can multiplex request on the same connection, but it does that as a cost of extra stuff. Now, if you take advantage of that, then the cost is apportioned. But if you front end sends one request and waits for another, there's no point of using HTTP 2. You just added an additional cost for nothing. So I always measure the need for HTTP 1 versus 2 based on that. Because I like to treat my CPU with respect. Just because we have something in abundance doesn't mean we have to waste. It's like going to a restaurant and ordering everything and only eating one meal and throwing the rest. Well, we have enough food. Let's just order. Well, respect your CPU. Decoding, the fifth step. This might actually happen before parsing. It depends on the protocol. But decoding essentially is like referring to what am I reading, the actual request itself. It's often happening after so that you need whatever you got as a request. Is this text? Is this binary? What is it really? And if it's text, is it ASCII? Is it UTF-8? Because the byte might look the same, but if it's ASCII, it might look completely different than if it's UTF-8. You might send an emoji. In ASCII, it doesn't mean anything. But it will show you maybe two letters. But in UTF-8, it means a dog. That's why decoding is very critical. That's just one space or decoding. And UTF-8 can take up to four bytes for certain characters. I think kanji can take up to four bytes for specific kanjis in Japanese. So yeah, drawing this, decoding is based on the language of the backend. Language here, I mean the human language, not the programming language, per se. And also, while not common, there's sometimes you compress requests, right? And you need to decode this compressed request such that if you compress the request with JISA, for example, the backend can decompress it. And that is another hit for CPU. So now we have three steps. So we're hitting the CPU, right? So we have the TLS. We have parsing and decoding as well. Take CPU, right? So we're talking to CPU and all these things. You might say these are just tiny stuff. Who cares? Why even measure it? Well, knowing about it actually doesn't hurt because if they happen on abundance and you did move your application to an IoT device and all of a sudden you say, oh, why is this application taking 100% CPU and everything is slow? Well, because you did all of this stuff that you didn't feel in your 128 core, whatever, CPU, right? Take it for granted. Final step. Now that we decoded, now we know the start and the end of the request. Now we fire the event. If there is such a thing in the backend process, I say, hey, a callback happened. I just got a request. Yay. I just got a request. Do whatever you want with it. But did you see all the stuff that happened? Now this is what we just see as backend. Okay, I just received a request. Now let me process this request. And even this step here as sometimes like there is, before you process the request, I'm glad also like node and express express JS does not do that for you. I'm really glad that it's actually explicitly said, Hey, this request contains a JSON. You parse that. So that's a different parsing from the parsing from step four, right? We get that request, but the request has a body and the body is of type Jason and it's only bytes, right? If you want those Jason bytes from normal bytes to actually something that you can work with in the application as objects, then you de serialize. That's a step that I didn't mention, right? So it can happen before processing, if you will. You can shove it in with decoding, if you will. I would de serialize this step, this, this Jason into Jason object such that I can call a dot whatever in it, right? I can play with it in JavaScript or in Python, you move this binaries into dictionaries, right? In Python is that what's called dictionaries, right? Python. That's what Jason is in Python. So you move these bytes into the applicable programming language data structure of your choice. If you're using node, then it's JavaScript, it's JSON, it's native JSON of the JavaScript. Then there will be serial de-serialization happening here. Boom, move it from a serialized byte down to an object that I can use. If it's Java, it's going to be a bunch of objects, right? And boy, this is really costly, especially if it depends on the language. There's a lot of overhead when it comes to JSON parsing and de-serializing. So that's another step. And then finally, once you get the Jason object, now you actually can process because now you know what you sent, what you were sent, and you can process it. And what does it mean to process? Well, you received a request to, I don't know, get slash books. Go give me the books, right? All right. Let me do a, now you turn around and do select star from books, right? Limit 10. Give me the first 10 books. And that's another. Now your backend becomes a client and the database becomes the backend. And you go through the exact same steps. Nothing changes. The fundamentals are the fundamentals are the fundamentals. And then once you process, you now go to the reverse of these steps, where you still do the, these steps, but they are in reverse, right? Where now you're writing, instead of reading, you have the connection, you're writing to the send queue before you write, you have to encrypt, right? And before you encrypt, you have to encode, right? So encode, decrypt, de-serialize, and then encrypt right to the send queue. Now the kernel will take care of that. It will send your encrypted by to the client and the client will do that, the reverse and so on. Right? So the processing aspects of things is actually also interesting because all these six steps, actually any one of them, you can really write, I'm not exaggerating, you can write a white paper and you can take your masters or PhD in each one of these steps, right? Cause like take the processing first, like how do you process requests? You have a fleet of requests arriving to your request. And I don't care about distributed. Don't involve the stuff now here. We're about simplicity. We're about learning what happens in a single machine. Let's not care about what multi-process and multi-distributed and stuff like that. Or one machine only, one machine. So how do you, to me, you should be as efficient as possible in one machine. Otherwise if you distribute too soon, to me it's a cop out. It's just like, well you just say, yeah, CPU 100%. More machines. Why is it 100%? Which of these steps are actually consuming more of your memory? Well, I don't care. Well, sure. Okay. Just throw more ramen, make the venture capitalist happy and then move on with your life. You can do that. Sure. But you can also be an engineer and understand these things. And that's what fascinates me. You know, I just sit down, wake up every day and I pick one thing that I think I know, but I turn out that I don't. Right. And, and they're just processing the thick thick of the pop, just processing requests. Right. Like you can have your own architecture to process requests. Right. Even if it ends up distributing to another machine, but it's just understanding that we have now a bunch of requests. I can spin up a pool of worker processes, five, six. What's this number? What does this number depend on? Well, it depends on the workload of the process. What is the nature of your request? This is definitely out of the scope of this video. Is it, is your, is your workload, is your request CPU bound or is it IO bound? Okay. Is your request, is your request traversing a one billion graph looking for something, which is CPU intensive thing, or is it reading from a database or reading from another service, or is it both where you are reading the graph that's IO and then you are traversing the graph. And so IO and CPU intensive. And that's how you scale. And that's how people pay or paid millions of dollars just to understand like how to scale things. You know, how do I size my machines? So yeah, that's what I'm interesting in, to be honest, is just understanding the bottleneck, right? Understanding the nature of your requests, what happens through all of that. I hope you enjoyed this episode and I'm going to see you in the next one. You guys stay awesome. Bye."}